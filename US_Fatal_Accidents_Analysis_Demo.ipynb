{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "US-Fatal-Accidents-Analysis-Demo",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npA2DeQAgFO5",
        "colab_type": "text"
      },
      "source": [
        "#Dataproc data pipeline to Analyse the fatal accidents happened in US "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVsekpN7Z6KJ",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "*   There are more than 35,000 fatal accidents every year in US, stated by FARS\n",
        "\n",
        "*   Detailing the factors behind traffic fatalities on the roads is critical\n",
        "\n",
        "## Dataset \n",
        "*   https://www.nhtsa.gov/research-data/fatality-analysis-reporting-system-fars\n",
        "\n",
        "\n",
        "*   https://www.nhtsa.gov/node/97996/121986 Traffic fatalities happened on 2016\n",
        "\n",
        "## Cluster\n",
        "*   Created 3 node DataProc cluster, 1 Master node and 2 Worker node with 2 cores and 7.5 GB memory\n",
        "\n",
        "*   Implemented the data pipeline using IAAC concepts\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQ9wGvlGgcaG",
        "colab_type": "text"
      },
      "source": [
        "##Setting up Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wLxv45fgYnH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "07ebc036-9226-47c7-f7a6-033518185d1a"
      },
      "source": [
        "# log in with the email account associated with your GCP.\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "print('Authenticated')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Authenticated\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJkUpmeQTops",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "project_id =  \"dataprocproject-275812\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "il8rLFT4wu-2",
        "colab_type": "text"
      },
      "source": [
        "##Set the Project"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZfVarmKwx1e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create service account credentials in colab env\n",
        "import os\n",
        "import json\n",
        "\n",
        "clientJson = {\n",
        "  \"type\": \"service_account\",\n",
        "  \"project_id\": project_id,\n",
        "  \"private_key_id\": \"af7f1acc7097ed7cd378fe0d85346ac9e6c6cd6b\",\n",
        "  \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nMIIEugIBADANBgkqhkiG9w0BAQEFAASCBKQwggSgAgEAAoIBAQC+cOpV+SdimPUo\\nJcU9qs9ihvTgqaihnCWpAODFBEdQlQPY6aStxOh26uCU1ZuOmG71l+w/wzqMRArW\\nWNexUtC6rYOw/BXgZAYHG5KsP/i0evMpI11M0fuS0rlyDwx/U+swvK2Dx7SvE9h9\\nXMr463RxdlyZ4ve5YYoL7xvJpHKGAxjjyD2s6MDxQ98znOS4cKi3M27T9cHBerDm\\nIv6L1t2urAnohlNHiunslHfy3xJ0YwDaHlxQi0u6/1iXKfazg1iO2v89di0vc4Rb\\nmAtiHNKPHuvKejohu8Fwn62OkqN43dnR8n0RwEqvDRM8jhI+3HB+ebkKrNlj8dwl\\n8i8rvVfDAgMBAAECgf9ws2H6fvhP4TU0DSYs2GjiZfuE+OJSwZWH7n0WGvIZPhaI\\nx7wtGdp4hYdijQcB3DPOI5osId0EJUBEmU8MIXZlA0pr8sL/iDIJAshKJ5GHPTup\\nnMQaMiovOYWFemJq9763mPEWi5jiA1ya089KgAzpGKK0bx0rGA4aAebv7eHYa/ab\\nzB1i3ZqJqTcnK1OPXJdyy2ci7UGlJOP6Bihlmw5RI9/pDcTKRgd8LWIcDyXhuqN4\\nxKaKIxHIbCx3yGLLwN/KI7XeTcD4AXrQW5JiCowY4Qrwur+ywgxH691Dz09L/kxu\\n6XvxtcFG7vZK/CilauRG8KrfGy3jwKfFnEhvQX0CgYEA73eypzPWrp+AWA8TsrN4\\nsq1Taka+FA4rcePBLGkxtFZcMlqxzN0k2SqHswOZU8vDnANkcbp+r8M9ZqTMQagN\\nddpAAxgjZgto1FQ6Wfwpc5sqRKyDkRNrJIB6qYWaFDnLY2YOxOyTeUrmlUF77G1k\\nkJjMZfz7fANWnlxihVBUF38CgYEAy5a7pw9+O+HKtDxjNdtAXW90H7ReuWhIbUYe\\nALzQVJCWfRDof+uYOAfTiich0wklsjqUiOeEZgfS6ocjqD0SvyXL/Po12GxDYcJc\\nMkJGSZjVJ9g5FisHiXCebwVME7aIlc5il3AKr54hBMefUzhqSt5bdtth3B35ytd5\\nmpKvgb0CgYBVyXXQ/HsuN6nM4inWXQEiUYGOkyVJHgJmXIc5l73cswt4ZpFHqPmo\\natoOQX3OTTMYUwbKz1rPMIxwlx0tcDzSOYuxEBauWewGFN6KmhFnfvDXhU3kzSYW\\n/4fx7xDRqhq40zj7Rx1V2IVedwUkH9RlELg42TZmqPOA6+vHgo19lQKBgE05SMbO\\ntqhKNlR1j4BPQTPv4dFg3olfiBGDBkBrXm6lRZ/8+OQE3oFqXpBJ1aeyEY1wcQUZ\\nJHx7PULereTT/bdpw92anzAPqON9EPGzd+vUwA3UE5tYCQzWHhKopLnZhCem2Hju\\n8+63gXjJq6KqhI8PlwwXnSAccrLBVwwN1erVAoGAP0yMDfilhpYSg20v8U7OnryK\\nWTrRGc+JiD1mLFE19nXMFlBzvvQyl4+5N1WbHp/E8NptQSUnisgN8Xmk1ca487pB\\nuZrWr9CU/Smg/BE+hWIjdmZhhsHiEJ/bduFA9/qPlkj6qmv4n6YNSeuu/kgoMMqG\\nm7dA3PYIbQZzNb3dOFc=\\n-----END PRIVATE KEY-----\\n\",\n",
        "  \"client_email\": \"demoserviceaccount@dataprocproject-275812.iam.gserviceaccount.com\",\n",
        "  \"client_id\": \"113604048937970872622\",\n",
        "  \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n",
        "  \"token_uri\": \"https://oauth2.googleapis.com/token\",\n",
        "  \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n",
        "  \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/demoserviceaccount%40dataprocproject-275812.iam.gserviceaccount.com\"\n",
        "}\n",
        "\n",
        "with open('client_secret.json', 'w', encoding='utf-8') as outfile:\n",
        "    json.dump(clientJson, outfile, ensure_ascii=False, indent=2)\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OlJScIJThd_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"./client_secret.json\"\n",
        "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = project_id\n",
        "os.environ[\"CLOUDSDK_CORE_PROJECT\"] = project_id"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ME5nMdSTDnn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "246b07f1-0812-4963-a497-bf65835ca648"
      },
      "source": [
        "!ls "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "adc.json\t    copy-jars.sh\t    us_accidents_data_pipeline.py\n",
            "client_secret.json  FatalAccidents2016.csv\n",
            "CodeOfStates.csv    sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvKvJ3MQu4cC",
        "colab_type": "text"
      },
      "source": [
        "##Get Fatal Accident data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOsSDej5uzBS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "508d4d2b-4f0e-4c1f-8f35-ab49f4c379da"
      },
      "source": [
        "!rm Fatal*.*\n",
        "!rm Code*.*\n",
        "!rm copy*.*\n",
        "!wget -q https://www.dropbox.com/s/wb8sanapqye8jas/FatalAccidents2016.csv\n",
        "!wget -q https://www.dropbox.com/s/k3pifq88p3dsu94/CodeOfStates.csv\n",
        "!wget -q https://www.dropbox.com/s/ymjoqor2jnfok3r/copy-jars.sh\n",
        "!ls -ltr"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 5260\n",
            "drwxr-xr-x 1 root root    4096 Jun 26 16:26 sample_data\n",
            "-rw-r--r-- 1 root root    2683 Jul  1 10:45 adc.json\n",
            "-rw-r--r-- 1 root root    5568 Jul  1 11:49 us_accidents_data_pipeline.py\n",
            "-rw-r--r-- 1 root root    2352 Jul  1 12:27 client_secret.json\n",
            "-rw-r--r-- 1 root root 5356665 Jul  1 12:28 FatalAccidents2016.csv\n",
            "-rw-r--r-- 1 root root     844 Jul  1 12:28 CodeOfStates.csv\n",
            "-rw-r--r-- 1 root root     110 Jul  1 12:28 copy-jars.sh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJpUqBU4vf9Y",
        "colab_type": "text"
      },
      "source": [
        "## Create GCS Bucket"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rfo-5MKBvmiJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from googleapiclient.discovery import build\n",
        "gcs_service = build('storage', 'v1')\n",
        "\n",
        "# Generate a random bucket name to which we'll upload the file.\n",
        "#import uuid\n",
        "bucket_name = 'ararivolidemobucket'# + str(uuid.uuid1())\n",
        "\n",
        "body = {\n",
        "  'name': bucket_name,\n",
        "  # For a full list of locations, see:\n",
        "  # https://cloud.google.com/storage/docs/bucket-locations\n",
        "  'location': 'us',\n",
        "}\n",
        "gcs_service.buckets().insert(project=project_id, body=body).execute()\n",
        "print('Done')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwhSibM-8hHu",
        "colab_type": "text"
      },
      "source": [
        "## Upload Fatal Dataset to GCS Bucket\n",
        "\n",
        "1.   FatalAccidents2016.csv\n",
        "2.   CodeOfStates.csv\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TIdRD3u0teL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from googleapiclient.http import MediaFileUpload\n",
        "\n",
        "media = MediaFileUpload('CodeOfStates.csv', \n",
        "                        mimetype='text/plain',\n",
        "                        resumable=True)\n",
        "\n",
        "request = gcs_service.objects().insert(bucket=bucket_name, \n",
        "                                       name='CodeOfStates.csv',\n",
        "                                       media_body=media)\n",
        "\n",
        "response = None\n",
        "while response is None:\n",
        "  # _ is a placeholder for a progress object that we ignore.\n",
        "  # (Our file is small, so we skip reporting progress.)\n",
        "  _, response = request.next_chunk()\n",
        "\n",
        "print('Upload complete')\n",
        "print('https://console.cloud.google.com/storage/browser?project={}'.format(project_id))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtYr58Rz1GVL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "media = MediaFileUpload('FatalAccidents2016.csv', \n",
        "                        mimetype='text/plain',\n",
        "                        resumable=True)\n",
        "\n",
        "request = gcs_service.objects().insert(bucket=bucket_name, \n",
        "                                       name='FatalAccidents2016.csv',\n",
        "                                       media_body=media)\n",
        "\n",
        "response = None\n",
        "while response is None:\n",
        "  # _ is a placeholder for a progress object that we ignore.\n",
        "  # (Our file is small, so we skip reporting progress.)\n",
        "  _, response = request.next_chunk()\n",
        "\n",
        "print('Upload complete')\n",
        "print('https://console.cloud.google.com/storage/browser?project={}'.format(project_id))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZ3mHbNFFQe_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "media = MediaFileUpload('copy-jars.sh', \n",
        "                        mimetype='text/plain',\n",
        "                        resumable=True)\n",
        "\n",
        "request = gcs_service.objects().insert(bucket='jarsforbigquery', \n",
        "                                       name='copy-jars.sh',\n",
        "                                       media_body=media)\n",
        "\n",
        "response = None\n",
        "while response is None:\n",
        "  # _ is a placeholder for a progress object that we ignore.\n",
        "  # (Our file is small, so we skip reporting progress.)\n",
        "  _, response = request.next_chunk()\n",
        "\n",
        "print('Upload complete')\n",
        "print('https://console.cloud.google.com/storage/browser?project={}'.format(project_id))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eoTV72LCZpI",
        "colab_type": "text"
      },
      "source": [
        "## Create DataProc Cluster"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shrsKmbaWU-5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "outputId": "bb27c28b-6f90-4790-f100-57644fb4a1a6"
      },
      "source": [
        " !pip3 install --upgrade google-cloud-storage"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: google-cloud-storage in /usr/local/lib/python3.6/dist-packages (1.29.0)\n",
            "Requirement already satisfied, skipping upgrade: google-cloud-core<2.0dev,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-storage) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: google-resumable-media<0.6dev,>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-storage) (0.5.1)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2.0dev,>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-storage) (1.17.2)\n",
            "Requirement already satisfied, skipping upgrade: google-api-core<2.0.0dev,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage) (1.16.0)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from google-resumable-media<0.6dev,>=0.5.0->google-cloud-storage) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage) (47.3.1)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage) (4.1.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage) (4.6)\n",
            "Requirement already satisfied, skipping upgrade: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage) (1.52.0)\n",
            "Requirement already satisfied, skipping upgrade: pytz in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage) (3.12.2)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.11.0->google-cloud-storage) (0.4.8)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage) (2.9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJgbNv4XX9LZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "outputId": "0bd90534-bc8f-4fda-a8d8-6c0a4230d79a"
      },
      "source": [
        "!pip3 uninstall protobuf\n",
        "!pip3 install protobuf"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling protobuf-3.12.2:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.6/dist-packages/google/protobuf/*\n",
            "    /usr/local/lib/python3.6/dist-packages/protobuf-3.12.2-py3.6-nspkg.pth\n",
            "    /usr/local/lib/python3.6/dist-packages/protobuf-3.12.2.dist-info/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled protobuf-3.12.2\n",
            "Collecting protobuf\n",
            "  Using cached https://files.pythonhosted.org/packages/28/05/9867ef8eafd12265267bee138fa2c46ebf34a276ea4cbe184cba4c606e8b/protobuf-3.12.2-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf) (47.3.1)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf) (1.12.0)\n",
            "\u001b[31mERROR: google-cloud-bigquery 1.21.0 has requirement google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1, but you'll have google-resumable-media 0.5.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: protobuf\n",
            "Successfully installed protobuf-3.12.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TeikjyaWpif",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "f4bccf92-f217-4e8f-e80b-b7e65e852f4f"
      },
      "source": [
        "!pip install google-cloud-dataproc"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: google-cloud-dataproc in /usr/local/lib/python3.6/dist-packages (1.0.0)\n",
            "Requirement already satisfied: google-api-core[grpc]<2.0.0dev,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-dataproc) (1.16.0)\n",
            "Requirement already satisfied: google-auth<2.0dev,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-dataproc) (1.17.2)\n",
            "Requirement already satisfied: setuptools>=34.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-dataproc) (47.3.1)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-dataproc) (3.12.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-dataproc) (1.12.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-dataproc) (1.52.0)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-dataproc) (2.23.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-dataproc) (2018.9)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.8.2; extra == \"grpc\" in /usr/local/lib/python3.6/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-dataproc) (1.30.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2.0dev,>=0.4.0->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-dataproc) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2.0dev,>=0.4.0->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-dataproc) (4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2.0dev,>=0.4.0->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-dataproc) (4.1.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-dataproc) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-dataproc) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-dataproc) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-dataproc) (2.9)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=0.4.0->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-dataproc) (0.4.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqoKiOoHS8_p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "import sys\n",
        "import argparse\n",
        "import os\n",
        "\n",
        "from google.cloud import dataproc_v1 as dproc\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7p-fqkjpEfSx",
        "colab_type": "text"
      },
      "source": [
        "## Create Data Proc Cluster"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Wt9UIR-BGqt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#  !gcloud beta dataproc clusters create demodataproccluster --enable-component-gateway --region us-central1 \\\n",
        "#   --subnet default --zone us-central1-a --single-node --master-machine-type n1-standard-8 --master-boot-disk-size 500\\\n",
        "#    --image-version 1.4-debian9   --initialization-actions gs://jarsforbigquery/copy-jars.sh\n",
        "\n",
        "\n",
        "# !gcloud beta dataproc clusters create demodataproccluster \\\n",
        "#     --enable-component-gateway \\\n",
        "#     --region us-central1 \\\n",
        "#     --subnet default \\\n",
        "#     --zone us-central1-b \\\n",
        "#     --master-machine-type n1-standard-2 \\\n",
        "#     --master-boot-disk-size 500 \\\n",
        "#     --num-workers 2 \\\n",
        "#     --worker-machine-type n1-standard-2 \\\n",
        "#     --worker-boot-disk-size 500 \\\n",
        "#     --image-version 1.4-debian9 \\\n",
        "#     --max-idle 10800s \\\n",
        "#     --project dataprocproject-275812 \\\n",
        "#     --optional-components ANACONDA,JUPYTER \\\n",
        "#     --initialization-actions gs://jarsforbigquery/copy-jars.sh\n",
        "\n",
        "\n",
        "# !gcloud beta dataproc clusters create demodataproccluster --enable-component-gateway --region us-central1 \\\n",
        "#   --subnet default --zone us-central1-a --single-node --master-machine-type n1-standard-8 --master-boot-disk-size 500\\\n",
        "#    --image-version 1.4-debian9   --initialization-actions gs://jarsforbigquery/copy-jars.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGNGAVui_29b",
        "colab_type": "text"
      },
      "source": [
        "## Create Data Proc Cluster"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hc5DL1X5Cdd7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fe8e35b4-31d3-42c9-924b-e1bd9cf8cf54"
      },
      "source": [
        "from google.cloud import dataproc_v1 as dproc\n",
        "\n",
        "\n",
        "def create_cluster(project_id, region, cluster_name):\n",
        "    \"\"\"This sample walks a user through creating a Cloud Dataproc cluster\n",
        "       using the Python client library.\n",
        "\n",
        "       Args:\n",
        "           project_id (string): Project to use for creating resources.\n",
        "           region (string): Region where the resources should live.\n",
        "           cluster_name (string): Name to use for creating a cluster.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create a client with the endpoint set to the desired cluster region.\n",
        "    cluster_client = dproc.ClusterControllerClient(client_options={\n",
        "        'api_endpoint': f'{region}-dataproc.googleapis.com:443',\n",
        "    })\n",
        "\n",
        "    # Create the cluster config.\n",
        "    cluster = {\n",
        "        'project_id': project_id,\n",
        "        'cluster_name': cluster_name,\n",
        "        'config': {\n",
        "            'master_config': {\n",
        "                'num_instances': 1,\n",
        "                'machine_type_uri': 'n1-standard-1'\n",
        "            },\n",
        "            'worker_config': {\n",
        "                'num_instances': 2,\n",
        "                'machine_type_uri': 'n1-standard-1'\n",
        "            },\n",
        "            \"initialization_actions\": [\n",
        "                {\n",
        "                  \"executable_file\": \"gs://jarsforbigquery/copy-jars.sh\"\n",
        "                }\n",
        "          ],\n",
        "          'software_config': {\n",
        "                         # I've tried the following:\n",
        "                        'optional_components': ['ANACONDA','JUPYTER']\n",
        "                },\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Create the cluster.\n",
        "    operation = cluster_client.create_cluster(project_id, region, cluster)\n",
        "    result = operation.result()\n",
        "\n",
        "    # Output a success message.\n",
        "    print(f'Cluster created successfully: {result.cluster_name}')\n",
        "\n",
        "region = 'us-central1'\n",
        "project_id = 'dataprocproject-275812'\n",
        "cluster_name = 'demodataproccluster'\n",
        "create_cluster(project_id, region, cluster_name)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cluster created successfully: demodataproccluster\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cNv71knGuKe",
        "colab_type": "text"
      },
      "source": [
        "##Read accident datasets from GCS and Analyse and store in BigQiery table\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyW3gwG5GPBw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "27f02523-e9d1-407f-93de-b72cf4ec77e4"
      },
      "source": [
        "!gsutil cp  gs://jarsforbigquery/us_accidents_data_pipeline.py us_accidents_data_pipeline.py"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://jarsforbigquery/us_accidents_data_pipeline.py...\n",
            "/ [1 files][  5.4 KiB/  5.4 KiB]                                                \n",
            "Operation completed over 1 objects/5.4 KiB.                                      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eiz33AQAGUGC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "47fd8636-9dac-49ab-cd7a-9c4dc67f0a0b"
      },
      "source": [
        "!gcloud dataproc jobs submit pyspark \\\n",
        "       --cluster demodataproccluster \\\n",
        "       --region us-central1 \\\n",
        "       us_accidents_data_pipeline.py \\\n",
        "       -- --bucket='gs://bucketfortemp'"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Job [6c55166fc7f64110921d14f3d7a81eb4] submitted.\n",
            "Waiting for job output...\n",
            "20/07/01 12:34:14 INFO org.spark_project.jetty.util.log: Logging initialized @5682ms\n",
            "20/07/01 12:34:14 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown\n",
            "20/07/01 12:34:14 INFO org.spark_project.jetty.server.Server: Started @5874ms\n",
            "20/07/01 12:34:14 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@4cb6fc35{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n",
            "20/07/01 12:34:14 WARN org.apache.spark.scheduler.FairSchedulableBuilder: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.\n",
            "20/07/01 12:34:17 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at demodataproccluster-m/10.128.0.44:8032\n",
            "20/07/01 12:34:17 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at demodataproccluster-m/10.128.0.44:10200\n",
            "20/07/01 12:34:22 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1593606728185_0001\n",
            "20/07/01 12:34:56 WARN org.apache.spark.util.Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.\n",
            "root\n",
            " |-- STATE: integer (nullable = true)\n",
            " |-- ST_CASE: integer (nullable = true)\n",
            " |-- VE_TOTAL: integer (nullable = true)\n",
            " |-- VE_FORMS: integer (nullable = true)\n",
            " |-- PVH_INVL: integer (nullable = true)\n",
            " |-- PEDS: integer (nullable = true)\n",
            " |-- PERNOTMVIT: integer (nullable = true)\n",
            " |-- PERMVIT: integer (nullable = true)\n",
            " |-- PERSONS: integer (nullable = true)\n",
            " |-- COUNTY: integer (nullable = true)\n",
            " |-- CITY: integer (nullable = true)\n",
            " |-- DAY: integer (nullable = true)\n",
            " |-- MONTH: integer (nullable = true)\n",
            " |-- YEAR: integer (nullable = true)\n",
            " |-- DAY_WEEK: integer (nullable = true)\n",
            " |-- HOUR: integer (nullable = true)\n",
            " |-- MINUTE: integer (nullable = true)\n",
            " |-- NHS: integer (nullable = true)\n",
            " |-- RUR_URB: integer (nullable = true)\n",
            " |-- FUNC_SYS: integer (nullable = true)\n",
            " |-- RD_OWNER: integer (nullable = true)\n",
            " |-- ROUTE: integer (nullable = true)\n",
            " |-- TWAY_ID: string (nullable = true)\n",
            " |-- TWAY_ID2: string (nullable = true)\n",
            " |-- MILEPT: integer (nullable = true)\n",
            " |-- LATITUDE: double (nullable = true)\n",
            " |-- LONGITUD: double (nullable = true)\n",
            " |-- SP_JUR: integer (nullable = true)\n",
            " |-- HARM_EV: integer (nullable = true)\n",
            " |-- MAN_COLL: integer (nullable = true)\n",
            " |-- RELJCT1: integer (nullable = true)\n",
            " |-- RELJCT2: integer (nullable = true)\n",
            " |-- TYP_INT: integer (nullable = true)\n",
            " |-- WRK_ZONE: integer (nullable = true)\n",
            " |-- REL_ROAD: integer (nullable = true)\n",
            " |-- LGT_COND: integer (nullable = true)\n",
            " |-- WEATHER1: integer (nullable = true)\n",
            " |-- WEATHER2: integer (nullable = true)\n",
            " |-- WEATHER: integer (nullable = true)\n",
            " |-- SCH_BUS: integer (nullable = true)\n",
            " |-- RAIL: string (nullable = true)\n",
            " |-- NOT_HOUR: integer (nullable = true)\n",
            " |-- NOT_MIN: integer (nullable = true)\n",
            " |-- ARR_HOUR: integer (nullable = true)\n",
            " |-- ARR_MIN: integer (nullable = true)\n",
            " |-- HOSP_HR: integer (nullable = true)\n",
            " |-- HOSP_MN: integer (nullable = true)\n",
            " |-- CF1: integer (nullable = true)\n",
            " |-- CF2: integer (nullable = true)\n",
            " |-- CF3: integer (nullable = true)\n",
            " |-- FATALS: integer (nullable = true)\n",
            " |-- DRUNK_DR: integer (nullable = true)\n",
            "\n",
            "20/07/01 12:35:12 INFO com.google.cloud.spark.bigquery.BigQueryWriteHelper: Submitted load to GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=us_fatal_accidents, projectId=dataprocproject-275812, tableId=us_fatal_accidents_2016}}. jobId: JobId{project=dataprocproject-275812, job=367ee315-2520-4098-801c-af594d255920, location=US}\n",
            "20/07/01 12:35:22 INFO com.google.cloud.spark.bigquery.BigQueryWriteHelper: Done loading to dataprocproject-275812.us_fatal_accidents.us_fatal_accidents_2016. jobId: JobId{project=dataprocproject-275812, job=367ee315-2520-4098-801c-af594d255920, location=US}\n",
            "20/07/01 12:35:25 INFO com.google.cloud.spark.bigquery.BigQueryWriteHelper: Submitted load to GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=us_fatal_accidents, projectId=dataprocproject-275812, tableId=us_states_code}}. jobId: JobId{project=dataprocproject-275812, job=46f28ccd-2d4a-41f6-b3e0-8456138be859, location=US}\n",
            "20/07/01 12:35:32 INFO com.google.cloud.spark.bigquery.BigQueryWriteHelper: Done loading to dataprocproject-275812.us_fatal_accidents.us_states_code. jobId: JobId{project=dataprocproject-275812, job=46f28ccd-2d4a-41f6-b3e0-8456138be859, location=US}\n",
            "+--------------+---------+\n",
            "|    state_name|acc_count|\n",
            "+--------------+---------+\n",
            "|    California|     3540|\n",
            "|         Texas|     3427|\n",
            "|       Florida|     2935|\n",
            "|       Georgia|     1424|\n",
            "|North Carolina|     1348|\n",
            "|  Pennsylvania|     1088|\n",
            "|          Ohio|     1053|\n",
            "|      Illinois|     1000|\n",
            "|      Michigan|      981|\n",
            "|      New York|      981|\n",
            "+--------------+---------+\n",
            "\n",
            "== Physical Plan ==\n",
            "TakeOrderedAndProject(limit=10, orderBy=[acc_count#1076L DESC NULLS LAST], output=[state_name#43,acc_count#1076L])\n",
            "+- *(3) HashAggregate(keys=[state_name#43], functions=[count(1)])\n",
            "   +- Exchange hashpartitioning(state_name#43, 200)\n",
            "      +- *(2) HashAggregate(keys=[state_name#43], functions=[partial_count(1)])\n",
            "         +- *(2) Project [state_name#43]\n",
            "            +- *(2) BroadcastHashJoin [state_number#45], [STATE#788], Inner, BuildLeft\n",
            "               :- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[1, int, true] as bigint)))\n",
            "               :  +- *(1) Project [state_name#43, state_number#45]\n",
            "               :     +- *(1) Filter isnotnull(state_number#45)\n",
            "               :        +- *(1) FileScan parquet [state_name#43,state_number#45] Batched: true, Format: Parquet, Location: InMemoryFileIndex[gs://ararivolidemobucket/statescodeparquet], PartitionFilters: [], PushedFilters: [IsNotNull(state_number)], ReadSchema: struct<state_name:string,state_number:int>\n",
            "               +- *(2) Project [STATE#788]\n",
            "                  +- *(2) Filter isnotnull(STATE#788)\n",
            "                     +- *(2) FileScan parquet [STATE#788] Batched: true, Format: Parquet, Location: InMemoryFileIndex[gs://ararivolidemobucket/accidentsparquet], PartitionFilters: [], PushedFilters: [IsNotNull(STATE)], ReadSchema: struct<STATE:int>\n",
            "root\n",
            " |-- state_name: string (nullable = true)\n",
            " |-- acc_count: long (nullable = false)\n",
            "\n",
            "20/07/01 12:35:45 INFO com.google.cloud.spark.bigquery.BigQueryWriteHelper: Submitted load to GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=us_fatal_accidents, projectId=dataprocproject-275812, tableId=accident_count}}. jobId: JobId{project=dataprocproject-275812, job=9e54a22f-e5f0-464d-bd93-eb26b3b40e67, location=US}\n",
            "20/07/01 12:35:53 INFO com.google.cloud.spark.bigquery.BigQueryWriteHelper: Done loading to dataprocproject-275812.us_fatal_accidents.accident_count. jobId: JobId{project=dataprocproject-275812, job=9e54a22f-e5f0-464d-bd93-eb26b3b40e67, location=US}\n",
            "20/07/01 12:35:53 INFO com.google.cloud.spark.bigquery.direct.DirectBigQueryRelation: Querying table dataprocproject-275812.us_fatal_accidents.accident_count, parameters sent from Spark: requiredColumns=[], filters=[]\n",
            "20/07/01 12:35:53 INFO com.google.cloud.spark.bigquery.direct.DirectBigQueryRelation: Going to read from dataprocproject-275812.us_fatal_accidents.accident_count columns=[], filter=''\n",
            "Results write to db finished.\n",
            "20/07/01 12:35:54 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@4cb6fc35{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n",
            "Job [6c55166fc7f64110921d14f3d7a81eb4] finished successfully.\n",
            "done: true\n",
            "driverControlFilesUri: gs://dataproc-staging-us-central1-291614269986-6rcuw8uu/google-cloud-dataproc-metainfo/7510a9cc-b366-4743-ba5e-f6e58e2fe48d/jobs/6c55166fc7f64110921d14f3d7a81eb4/\n",
            "driverOutputResourceUri: gs://dataproc-staging-us-central1-291614269986-6rcuw8uu/google-cloud-dataproc-metainfo/7510a9cc-b366-4743-ba5e-f6e58e2fe48d/jobs/6c55166fc7f64110921d14f3d7a81eb4/driveroutput\n",
            "jobUuid: 36c9f17d-ae46-38e5-a830-ab58978cd3e9\n",
            "placement:\n",
            "  clusterName: demodataproccluster\n",
            "  clusterUuid: 7510a9cc-b366-4743-ba5e-f6e58e2fe48d\n",
            "pysparkJob:\n",
            "  args:\n",
            "  - --bucket=gs://bucketfortemp\n",
            "  mainPythonFileUri: gs://dataproc-staging-us-central1-291614269986-6rcuw8uu/google-cloud-dataproc-metainfo/7510a9cc-b366-4743-ba5e-f6e58e2fe48d/jobs/6c55166fc7f64110921d14f3d7a81eb4/staging/us_accidents_data_pipeline.py\n",
            "reference:\n",
            "  jobId: 6c55166fc7f64110921d14f3d7a81eb4\n",
            "  projectId: dataprocproject-275812\n",
            "status:\n",
            "  state: DONE\n",
            "  stateStartTime: '2020-07-01T12:35:55.760Z'\n",
            "statusHistory:\n",
            "- state: PENDING\n",
            "  stateStartTime: '2020-07-01T12:34:06.858Z'\n",
            "- state: SETUP_DONE\n",
            "  stateStartTime: '2020-07-01T12:34:06.923Z'\n",
            "- details: Agent reported job success\n",
            "  state: RUNNING\n",
            "  stateStartTime: '2020-07-01T12:34:07.225Z'\n",
            "yarnApplications:\n",
            "- name: us_accidents_data_pipeline_demo\n",
            "  progress: 1.0\n",
            "  state: FINISHED\n",
            "  trackingUrl: http://demodataproccluster-m:8088/proxy/application_1593606728185_0001/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lT3BWO4HvzU4",
        "colab_type": "text"
      },
      "source": [
        "#Cleaning up the project environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoBc279L1UJL",
        "colab_type": "text"
      },
      "source": [
        "## Delete DemoBucket from GCS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bK4XiblC18Bp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.cloud import storage\n",
        "\n",
        "def delete_bucket(bucket_name):\n",
        "    \"\"\"Deletes a bucket. The bucket must be empty.\"\"\"\n",
        "    # bucket_name = \"your-bucket-name\"\n",
        "\n",
        "    storage_client = storage.Client()\n",
        "\n",
        "    bucket = storage_client.get_bucket(bucket_name)\n",
        "    bucket.delete(force=True)\n",
        "\n",
        "    print(\"Bucket {} deleted\".format(bucket.name))\n",
        "delete_bucket(bucket_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHe9FZ-8LL9V",
        "colab_type": "text"
      },
      "source": [
        "## Delete DataProc Cluster"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Km3fJAbrLSk4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "9db397a9-512d-4139-c65e-b71ae11d9223"
      },
      "source": [
        "!gcloud beta dataproc clusters delete demodataproccluster --region=us-central1"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The cluster 'demodataproccluster' and all attached disks will be \n",
            "deleted.\n",
            "\n",
            "Do you want to continue (Y/n)?  y\n",
            "\n",
            "Waiting on operation [projects/dataprocproject-275812/regions/us-central1/operations/6cd1590b-55c6-32b2-860a-4dfb34a3360c].\n",
            "Deleted [https://dataproc.googleapis.com/v1beta2/projects/dataprocproject-275812/regions/us-central1/clusters/demodataproccluster].\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}